---
title: "Netflix - Experimental Design"
output: github_document
---


```{r}
library(plot3D)
library(gplots)

# Function to create blues
blue_palette <- colorRampPalette(c(rgb(247,251,255,maxColorValue = 255), 
                                   rgb(8,48,107,maxColorValue = 255)))

# Function for converting from natural units to coded units
convert.N.to.C <- function(U,UH,UL){
  x <- (U - (UH+UL)/2) / ((UH-UL)/2)
  return(x)
}

# Function for converting from coded units to natural units
convert.C.to.N <- function(x,UH,UL){
  U <- x*((UH-UL)/2) + (UH+UL)/2
  return(U)
}
```

```{r}
setwd(dir = "/Users/shermeenvelani/Documents/Projects/Netflix - Experimental Design/data/")
Fact.Scr <- read.csv(file = "Phase1_Results_2^3.csv", header = T)

# Creating a data frame with the response variable and 3 factors in coded units.
# This data frame will be fed to the model
ph1 <- data.frame(y=Fact.Scr$Browse.Time,
                  TS = convert.N.to.C(Fact.Scr$Tile.Size,0.3,0.1),
                  PS = convert.N.to.C(Fact.Scr$Prev.Size,0.5,0.3),
                  PL = convert.N.to.C(Fact.Scr$Prev.Length,90,30))


# Linear regression

model <- lm(y~(TS+PS+PL)^3, data = ph1)
summary(model)
```

At the 5% significance level, we can see that Tile Size is not significant and
therefore can be removed. We are now left with 2 factors i.e. Preview Size
and Preview Length

The graphs below show the results obtained above


Plots

```{r}
par(mfrow=c(2,3))
plotmeans(formula = y~TS, ylab = "Average Browsing Time", xlab = "Tile.Size", 
          data = ph1, xaxt = "n", pch = 16,ylim = c(16,21))
axis(side = 1, at = c(1,2), labels = c("0.1", "0.3"))
plotmeans(formula = y~PS, ylab = "Average Browsing Time", xlab = "Prev.Size", 
          data = ph1, xaxt = "n", pch = 16,ylim = c(16,21))
axis(side = 1, at = c(1,2), labels = c("0.3", "0.5"))
plotmeans(formula = y~PL, ylab = "Average Browsing Time", xlab = "Prev.Length", 
          data = ph1, xaxt = "n", pch = 16,ylim = c(16,21))
axis(side = 1, at = c(1,2), labels = c("30", "90"))
```
```{r}
## Phase 2
## We have 2 factors now - Preview Size and Preview Length

setwd(dir = "/Users/shermeenvelani/Documents/Projects/Netflix - Experimental Design/data/")
netflix.ph2 <- read.csv("Phase2_2^2+cp.csv", header = TRUE)

## The factors and their low/center/high levels are as follows:
## Preview Length: 30  vs 60  vs 90
## Preview Size:   0.3 vs 0.4 vs 0.5


## Determine whethere we're close to the optimum to begin with
## (i.e, check whether the pure quadratic effect is significant)
ph2 <- data.frame(y = netflix.ph2$Browse.Time,
                  x1 = convert.N.to.C(U = netflix.ph2$Prev.Length, UH = 90, 
                                      UL = 30),
                  x2 = convert.N.to.C(U = netflix.ph2$Prev.Size, UH = 0.5, 
                                      UL = 0.3))
ph2$xPQ <- (ph2$x1^2 + ph2$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph2$y, by = list(x1 = ph2$x1, x2 = ph2$x2), FUN = mean)


## The difference in average browsing time in factorial conditions vs. the 
## center point condition
mean(ph2$y[ph2$xPQ != 0]) - mean(ph2$y[ph2$xPQ == 0])


## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph2)
summary(m)
```

We test the null hypothesis for coefficient of curvature being zero. 

Since the pvalue is 0.97 we fail to reject the null hypothesis and therefore
are not in the presence of quadratic curvature. To find the quadratic curvature,
we need to use the method of steepest descent

```{r}
## It isn't, so we're in a flat area of the responsr surface. We should
## perform a steepest descent phase.

## Fit the first order model to determine the direction of the path of 
## steepest descent
m.fo <- lm(y~x1+x2, data = ph2)
beta0 <- coef(m.fo)[1]
beta1 <- coef(m.fo)[2]
beta2 <- coef(m.fo)[3]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 90, UL = 30), 
                    convert.N.to.C(U = 120, UH = 90, UL = 30), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0.2, UH = 0.5, UL = 0.3), 
                    convert.N.to.C(U = 0.8, UH = 0.5, UL = 0.3), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.fo <- beta0 + beta1*x1 + beta2*x2

# 2D contour plot
contour(x = seq(convert.N.to.C(U = 30, UH = 90, UL = 30), 
                convert.N.to.C(U = 120, UH = 90, UL = 30), 
                length.out = 100),
        y = seq(convert.N.to.C(U = 0.2, UH = 0.5, UL = 0.3), 
                convert.N.to.C(U = 0.8, UH = 0.5, UL = 0.3), 
                length.out = 100), 
        z = eta.fo, xlab = "x1 (Preview Length)", ylab = "x2 (Preview Size)",
        nlevels = 15, col = blue_palette(15), labcex = 0.9, asp=1)
abline(a = 0, b = beta2/beta1, lty = 2)
points(x = 0, y = 0, col = "red", pch = 16)

# gradient
g <- matrix(c(beta1, beta2), nrow = 1)
g


### Now that we have a starting center point, we can start with the gradient
### descent

# We will take steps of size 10 seconds in preview length.In coded units this is
PL.step <- convert.N.to.C(U = 60 + 10, UH = 90, UL = 30)
lamda <- PL.step/abs(beta1)

## Step 0: The center point we've already observed
x.old <- matrix(0, nrow=1, ncol=2)
text(x = 0, y = 0+0.25, labels = "0")
step0 <- data.frame(Prev.Length = convert.C.to.N(x = 0, UH = 90, UL = 30), 
                 Prev.Size = convert.C.to.N(x = 0, UH = 0.5, UL = 0.3))

## Step 1: 
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "1")
step1 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, 
                                                 UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, 
                                               UL = 0.3))

## Step 2: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "2")
step2 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, 
                                                 UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, 
                                               UL = 0.3))

## Step 3: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "3")
step3 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, 
                                                 UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, 
                                               UL = 0.3))

## Step 4: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "4")
step4 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, 
                                                 UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, 
                                               UL = 0.3))

## Step 5: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step5 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, 
                                                 UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, 
                                               UL = 0.3))

## Step 6: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "6")
step6 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 90, 
                                                 UL = 30), 
                    Prev.Size = convert.C.to.N(x = x.new[1,2], UH = 0.5, 
                                               UL = 0.3))


## The following is a list of the conditions along the path of steepest descent
pstd.cond <- data.frame(Step = 0:6, rbind(step0, step1, step2, step3, step4, 
                                          step5, step6))
pstd.cond
```
```{r}
## We have now reached the final limit of Preview Length since our max length is
## 120 and so we know our optimal is in between the steps we calculated

## Load the data associated with the steepest descent search
setwd(dir = "/Users/shermeenvelani/Documents/Projects/Netflix - Experimental Design/data/")
grddescent <- read.csv("gradientdescent_steps.csv", header = TRUE)

## Calculate the average browsing time in each of these conditions and find the 
## condition that minimizes it
pstd.means <- aggregate(grddescent$Browse.Time, 
                        by = list(Prev.Length = grddescent$Prev.Length, 
                                  Prev.Size = grddescent$Prev.Size), 
                        FUN = mean)

plot(x = 0:6, y = pstd.means$x,
     type = "l", xlab = "Step Number", ylab = "Average Browsing Time")
points(x = 0:6, y = pstd.means$x,
       col = "red", pch = 16)

## Clearly average browsing time was minimized at Step 6
pstd.cond[pstd.cond$Step == 6,]
```

```{r}
## We should follow this up with 2^2 factorial conditions to ensure we're close 
## to the optimum
## We will re-center our coded scale in this new region as follows:
## Preview Length: 80  vs 90 vs 100
## Preview Size:   0.4577144 vs 0.48657165 vs 0.5154289

## Load this data and check whether the pure quadratic effect is significant
setwd(dir = "/Users/shermeenvelani/Documents/Projects/Netflix - Experimental Design/data/")
netflix.ph2.2 <- rbind(read.csv("2^2+cp_recentre.csv"),
                       grddescent[grddescent$Prev.Length == 90,])

ph4.5 <- data.frame(y = netflix.ph2.2$Browse.Time,
                    x1 = round(convert.N.to.C(U = netflix.ph2.2$Prev.Length, UH = 100, UL = 80),1),
                    x2 = round(convert.N.to.C(U = netflix.ph2.2$Prev.Size, UH = 0.5154289, UL = 0.4577144),1))
ph4.5$xPQ <- (ph4.5$x1^2 + ph4.5$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph4.5$y, by = list(x1 = ph4.5$x1, x2 = ph4.5$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph4.5$y[ph4.5$xPQ != 0]) - mean(ph4.5$y[ph4.5$xPQ == 0])

## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph4.5)
summary(m)
```
We can reject the null hypothesis which now means we are in 
the presence of a quadratic curvature. Hence, we can proceed to the next step
which is the Response Surface which fits a second order model to find the optimum

```{r}
## Phase 3
setwd(dir = "/Users/shermeenvelani/Documents/Projects/Netflix - Experimental Design/data/")
netflix.ph3 <- rbind(netflix.ph2.2,read.csv("Phase3_RSM.csv"))
ph3 <- data.frame(y = netflix.ph3$Browse.Time,
                  x1 = round(convert.N.to.C(U = netflix.ph3$Prev.Length, UH = 100, UL = 80),1),
                  x2 = round(convert.N.to.C(U = netflix.ph3$Prev.Size, UH = 0.5154289, UL = 0.4577144),1))

model <- lm(y ~ x1 + x2 + x1*x2 + I(x1^2) + I(x2^2), data = ph3)
summary(model)

```

```{r}
## Let's visualize this surface:
beta0 <- coef(model)[1]
beta1 <- coef(model)[2]
beta2 <- coef(model)[3]
beta12 <- coef(model)[6]
beta11 <- coef(model)[4]
beta22 <- coef(model)[5]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 100, UL = 80), 
                    convert.N.to.C(U = 120, UH = 100, UL = 80), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0.2, UH = 0.5154289, UL = 0.4577144), 
                    convert.N.to.C(U = 0.8, UH = 0.5154289, UL = 0.4577144), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.so <- beta0 + beta1*x1 + beta2*x2 + beta12*x1*x2 + beta11*x1^2 + beta22*x2^2


## Let's find the maximum of this surface and the corresponding factor levels 
## at which this is achieved
b <- matrix(c(beta1,beta2), ncol = 1)
B <- matrix(c(beta11, 0.5*beta12, 0.5*beta12, beta22), nrow = 2, ncol = 2)
x.s <- -0.5*solve(B) %*% b

# The predicted book rate at this configuration is:
eta.s <- beta0 + 0.5*t(x.s) %*% b
```

```{r}
# 2D contour plot (coded units)
par(mfrow = c(1,2))
contour(x = seq(convert.N.to.C(U = 30, UH = 100, UL = 80), 
                convert.N.to.C(U = 120, UH = 100, UL = 80), 
                length.out = 100), 
        y = seq(convert.N.to.C(U = 0.2, UH = 0.5154289, UL = 0.4577144), 
                convert.N.to.C(U = 0.8, UH = 0.5154289, UL = 0.4577144), 
                length.out = 100), 
        z = eta.so, xlab = "x1", ylab = "x2",
        nlevels = 20, col = blue_palette(20), labcex = 0.9)
## added optimal points calculated using natural to coded units function
points(x=-0.05097,y=1.078916,col="green",pch=16)
## units that can be implemented
points(x=0,y=0.4653371,col="red",pch=16)

# Remake the contour plot but in natural units
contour(x = seq(30, 120, length.out = 100), 
        y = seq(0.2, 0.8, length.out = 100), 
        z = eta.so, xlab = "x1 (Preview Length)", ylab = "x2 (Preview Size)",
        nlevels = 20, col = blue_palette(20), labcex = 0.9)
points(x = convert.C.to.N(x = x.s[1,1], UH = 100, UL = 80), 
       y = convert.C.to.N(x = x.s[2,1], UH = 0.5154289, UL = 0.4577144), 
       col = "green", pch = 16)
points(x = 90, y = 0.5, col = "red", pch = 16)
```

```{r}
# In natural units this optimum is located at
convert.C.to.N(x = x.s[1,1], UH = 100, UL = 80)
convert.C.to.N(x = x.s[2,1], UH = 0.5154289, UL = 0.4577144)


## 95% prediction interval at this optimum:
n.data <- data.frame(x1=x.s[1,1], x2=x.s[2,1])
pred <- predict(model, newdata = n.data, type = "response", se.fit = TRUE)
pred
print(paste("Prediction: ", pred$fit, sep = ""))
print(paste("95% Prediction interval: (", pred$fit-qnorm(0.975)*pred$se.fit, ",",
            pred$fit+qnorm(0.975)*pred$se.fit, ")", sep = ""))

```

